{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Titanic Survival Classification\n",
    "\n",
    "This notebook tackles the classic Kaggle competition: predicting passenger survival on the RMS Titanic. We will walk through the entire machine learning workflow:\n",
    "\n",
    "1.  **Exploratory Data Analysis (EDA):** Understanding the data and uncovering initial insights.\n",
    "2.  **Feature Engineering & Preprocessing:** Transforming raw data into a format suitable for machine learning models.\n",
    "3.  **Model Training:** Building and training Logistic Regression, Random Forest, and XGBoost models.\n",
    "4.  **Model Evaluation:** Comparing the models to see which performs best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Make sure the 'train.csv' and 'test.csv' files are in the 'data/' directory.\n",
    "try:\n",
    "    train_df = pd.read_csv('data/train.csv')\n",
    "    test_df = pd.read_csv('data/test.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Data files not found. Please download them from Kaggle and place them in the 'data/' directory.\")\n",
    "\n",
    "print(\"Train data shape:\", train_df.shape)\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's explore the dataset to understand its structure, find missing values, and visualize relationships between features and the survival outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of the training data\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print('Missing values in training data:\\n', train_df.isnull().sum())\\n",
    "print('\\n' + '-'*30 + '\\n')\\n",
    "print('Missing values in test data:\\n', test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Age`, `Cabin`, and `Embarked` columns have missing values in the training set. `Age` and `Cabin` also have missing values in the test set. We will need to handle these during preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Target Variable: Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='Survived', data=train_df)\n",
    "plt.title('Survival Count (0 = No, 1 = Yes)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Survival by Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "sns.countplot(x='Survived', hue='Sex', data=train_df, ax=axes[0])\n",
    "axes[0].set_title('Survival by Sex')\n",
    "\n",
    "sns.countplot(x='Survived', hue='Pclass', data=train_df, ax=axes[1])\n",
    "axes[1].set_title('Survival by Pclass')\n",
    "\n",
    "sns.countplot(x='Survived', hue='Embarked', data=train_df, ax=axes[2])\n",
    "axes[2].set_title('Survival by Embarked')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "*   **Sex:** Females had a much higher chance of survival.\n",
    "*   **Pclass:** Passengers in 1st class had a higher survival rate than those in 2nd and 3rd class.\n",
    "*   **Embarked:** Passengers who embarked at Cherbourg ('C') seem to have a higher survival rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.histplot(data=train_df, x='Age', hue='Survived', multiple='stack', kde=True, ax=axes[0])\n",
    "axes[0].set_title('Age Distribution by Survival')\n",
    "\n",
    "sns.histplot(data=train_df, x='Fare', hue='Survived', multiple='stack', kde=False, ax=axes[1])\n",
    "axes[1].set_title('Fare Distribution by Survival')\n",
    "axes[1].set_xlim(0, 200) # Limiting fare for better visualization\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "*   **Age:** Young children (age < 10) appear to have a higher survival rate. A large number of passengers aged 20-40 did not survive.\n",
    "*   **Fare:** Passengers who paid a higher fare had a better chance of survival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering & Preprocessing\n",
    "\n",
    "Now we'll prepare the data for modeling. This involves handling missing values, creating new features, and converting categorical data into a numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll process both train and test sets together to ensure consistency.\n",
    "# Let's keep the PassengerId from the test set for the final submission file.\n",
    "test_passenger_id = test_df['PassengerId']\n",
    "\n",
    "# We can drop PassengerId from the training set as it's not a feature.\n",
    "train_df = train_df.drop(['PassengerId'], axis=1)\n",
    "\n",
    "# Combine train and test data for easier processing\n",
    "all_df = pd.concat([train_df.drop('Survived', axis=1), test_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing 'Age' values with the median age.\n",
    "all_df['Age'] = all_df['Age'].fillna(all_df['Age'].median())\n",
    "\n",
    "# Fill missing 'Embarked' values with the mode (most frequent value).\n",
    "all_df['Embarked'] = all_df['Embarked'].fillna(all_df['Embarked'].mode()[0])\n",
    "\n",
    "# Fill missing 'Fare' in the test set with the median fare.\n",
    "all_df['Fare'] = all_df['Fare'].fillna(all_df['Fare'].median())\n",
    "\n",
    "# Drop the 'Cabin' column due to too many missing values.\n",
    "all_df = all_df.drop(['Cabin'], axis=1)\n",
    "\n",
    "print('Missing values after imputation:\\n', all_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'FamilySize' from 'SibSp' and 'Parch'.\n",
    "all_df['FamilySize'] = all_df['SibSp'] + all_df['Parch'] + 1\n",
    "\n",
    "# Create 'IsAlone' feature.\n",
    "all_df['IsAlone'] = 0\n",
    "all_df.loc[all_df['FamilySize'] == 1, 'IsAlone'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Categorical Features & Dropping Unused Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Sex' to numeric.\n",
    "all_df['Sex'] = all_df['Sex'].map({'male': 0, 'female': 1}).astype(int)\n",
    "\n",
    "# One-hot encode 'Embarked'.\n",
    "all_df = pd.get_dummies(all_df, columns=['Embarked'], prefix='Embarked')\n",
    "\n",
    "# Drop original columns that are now redundant or not useful.\n",
    "all_df = all_df.drop(['Name', 'Ticket', 'SibSp', 'Parch'], axis=1)\n",
    "\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating Data back into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the combined dataframe back into training and testing sets.\n",
    "X_train = all_df[:len(train_df)]\n",
    "# Drop PassengerId from X_test as it was not used for training\n",
    "X_test = all_df[len(train_df):].drop('PassengerId', axis=1)\n",
    "y_train = train_df['Survived']\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training & Evaluation\n",
    "\n",
    "It's time to train our models. We will use 5-fold cross-validation to evaluate three different classifiers and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "log_reg = LogisticRegression(max_iter=2000)\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "xgb = XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': log_reg,\n",
    "    'Random Forest': random_forest,\n",
    "    'XGBoost': xgb\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    results[name] = cv_scores\n",
    "    print(f'{name}: Mean Accuracy = {cv_scores.mean():.4f} (Std = {cv_scores.std():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=pd.DataFrame(results))\n",
    "plt.title('Model Accuracy Comparison (5-Fold Cross-Validation)')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "Random Forest and XGBoost perform similarly and are both stronger than Logistic Regression. Random Forest appears to have a slightly higher median accuracy in this run. For the final step, we can choose one of these to generate a submission file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model on the entire training dataset\n",
    "final_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = final_model.predict(X_test)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({'PassengerId': test_passenger_id, 'Survived': predictions})\n",
    "\n",
    "print('Submission file preview:')\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the file for submission to Kaggle:\n",
    "# submission_df.to_csv('titanic_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Findings\n",
    "\n",
    "This project aimed to predict passenger survival on the Titanic. Through our analysis, we confirmed several historical hypotheses:\n",
    "- Passengers in higher classes (`Pclass`) had a better chance of survival.\n",
    "- Female passengers (`Sex`) had a significantly higher survival rate than males.\n",
    "- We engineered a `FamilySize` feature and found that passengers who were alone (`IsAlone`) had a lower survival rate than those in small-to-medium-sized families.\n",
    "\n",
    "Among the three models tested, **Random Forest** and **XGBoost** were the top performers, both achieving a cross-validated accuracy of over 81%. Logistic Regression, while a good baseline, was clearly outperformed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "While the models perform reasonably well, there are several limitations to this analysis:\n",
    "1.  **Simple Imputation:** We used median/mode for imputation, which is simple but may not be the most accurate method.\n",
    "2.  **Feature Engineering:** Our feature engineering was basic. More complex features, like extracting titles from names (e.g., 'Mr.', 'Mrs.', 'Dr.'), could provide more signal.\n",
    "3.  **No Hyperparameter Tuning:** The models were trained with their default parameters. A systematic search for optimal hyperparameters would likely boost performance.\n",
    "4.  **Information Loss:** We dropped the `Cabin` column entirely. While it had many missing values, there might be a way to extract useful information from it (e.g., the deck level)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Next Steps\n",
    "\n",
    "To improve upon this project, one could:\n",
    "- **Advanced Feature Engineering:** Extract titles from the `Name` column and group rare titles.\n",
    "- **Hyperparameter Tuning:** Use `GridSearchCV` or `RandomizedSearchCV` to find the best settings for the Random Forest or XGBoost models.\n",
    "- **Ensemble Methods:** Create a stacked ensemble that combines the predictions of multiple models to potentially achieve higher accuracy.\n",
    "- **Error Analysis:** Perform a deeper analysis of the cases where our best model made incorrect predictions to understand its weaknesses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
