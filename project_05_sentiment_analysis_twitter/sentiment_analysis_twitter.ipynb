{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Sentiment Analysis on Twitter Data\n",
    "\n",
    "This notebook builds a deep learning model to classify the sentiment of tweets from the Sentiment140 dataset. The goal is to determine whether a tweet expresses a positive or negative sentiment.\n",
    "\n",
    "We will use a Long Short-Term Memory (LSTM) network, a type of Recurrent Neural Network (RNN), which is well-suited for sequence data like text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # The dataset has no header, so we provide column names\n",
    "    col_names = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "    df = pd.read_csv('data/training.1600000.processed.noemoticon.csv',\n",
    "                     encoding='latin-1',\n",
    "                     names=col_names)\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Data file not found. Please download and place it in the 'data/' directory.\")\n",
    "\n",
    "# Keep only the necessary columns\n",
    "df = df[['target', 'text']]\n",
    "\n",
    "# The target values are 0 (negative) and 4 (positive). Let's map 4 to 1.\n",
    "df['target'] = df['target'].replace(4, 1)\n",
    "\n",
    "print(\"\\nValue counts for sentiment:\")\n",
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "    \"\"\"Function to clean tweet text.\"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    # Remove user mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\u0041-\u005A\u0061-\u007A\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize and remove stopwords, and apply stemming\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            tokens.append(stemmer.stem(token))\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply the cleaning function\n",
    "# This may take a few minutes to run on the full dataset\n",
    "df['clean_text'] = df['text'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (Word Clouds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = \" \".join(df[df['target'] == 1]['clean_text'])\n",
    "negative_tweets = \" \".join(df[df['target'] == 0]['clean_text'])\n",
    "\n",
    "wordcloud_pos = WordCloud(width=800, height=400, background_color='white').generate(positive_tweets)\n",
    "wordcloud_neg = WordCloud(width=800, height=400, background_color='black').generate(negative_tweets)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(wordcloud_pos, interpolation='bilinear')\n",
    "plt.title('Most Common Words in Positive Tweets')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(wordcloud_neg, interpolation='bilinear')\n",
    "plt.title('Most Common Words in Negative Tweets')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering and Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X = df['clean_text'].values\n",
    "y = df['target'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_len = 50 # Max length of a sequence\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building and Training the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=128, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_pad, y_train,\n",
    "                    epochs=5, # Using 5 epochs for demonstration; more can improve accuracy\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_test_pad, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Model Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Get predictions\n",
    "y_pred_probs = model.predict(X_test_pad)\n",
    "y_pred = (y_pred_probs > 0.5).astype('int32')\n",
    "\n",
    "# Classification Report and Confusion Matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This notebook demonstrated a complete workflow for a common NLP task: sentiment analysis. We cleaned raw text data, converted it into a format suitable for a deep learning model, and trained an LSTM network to classify tweet sentiment with reasonable accuracy.\n",
    "\n",
    "### Potential Improvements\n",
    "- **Use Pre-trained Embeddings:** Instead of training our own embedding layer, we could use pre-trained embeddings like GloVe or Word2Vec, which are trained on much larger text corpora and can capture more semantic meaning.\n",
    "- **Try Different Architectures:** Experiment with GRUs (Gated Recurrent Units) or even more advanced models like Transformers (e.g., BERT).\n",
    "- **More Sophisticated Preprocessing:** Use techniques like lemmatization instead of stemming, or handle negations more carefully."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
